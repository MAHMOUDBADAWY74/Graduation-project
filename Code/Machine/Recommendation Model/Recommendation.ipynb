{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import packages & library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ“– ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒØªØ¨:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6/10 [02:14<01:29, 22.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error fetching metadata for 6: HTTPSConnectionPool(host='www.gutenberg.org', port=443): Max retries exceeded with url: /ebooks/6 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001A529EE7610>, 'Connection to www.gutenberg.org timed out. (connect timeout=30)'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ“– ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒØªØ¨:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 7/10 [02:35<01:05, 21.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error fetching metadata for 7: HTTPSConnectionPool(host='www.gutenberg.org', port=443): Max retries exceeded with url: /ebooks/7 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001A529EE7ED0>, 'Connection to www.gutenberg.org timed out. (connect timeout=30)'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ“– ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒØªØ¨: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [02:48<00:00, 16.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“š ØªÙ… Ø­ÙØ¸ 8 ÙƒØªØ§Ø¨ ÙÙŠ Ø§Ù„Ù…Ù„Ù: books_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'\n",
    "}\n",
    "TIMEOUT = 30\n",
    "\n",
    "def get_book_metadata(book_id):\n",
    "    try:\n",
    "        base_url = f\"https://www.gutenberg.org/ebooks/{book_id}\"\n",
    "        response = requests.get(base_url, headers=HEADERS, timeout=TIMEOUT)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        title = soup.find('h1', {'itemprop': 'name'}).get_text(strip=True) if soup.find('h1', {'itemprop': 'name'}) else f\"Untitled {book_id}\"\n",
    "        author = soup.find('a', {'itemprop': 'creator'}).get_text(strip=True) if soup.find('a', {'itemprop': 'creator'}) else \"Unknown\"\n",
    "        \n",
    "        cover_img = soup.find('img', {'itemprop': 'image'})\n",
    "        if cover_img:\n",
    "            cover_url = cover_img['src']\n",
    "            if not cover_url.startswith('http'):\n",
    "                cover_url = f\"https://www.gutenberg.org{cover_url}\"\n",
    "        else:\n",
    "            cover_url = f\"https://www.gutenberg.org/cache/epub/{book_id}/pg{book_id}.cover.medium.jpg\"\n",
    "\n",
    "        return title, author, cover_url\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error fetching metadata for {book_id}: {str(e)}\")\n",
    "        return None, None, None\n",
    "\n",
    "def get_book_text(book_id):\n",
    "    text_urls = [\n",
    "        f\"https://www.gutenberg.org/files/{book_id}/{book_id}-0.txt\",\n",
    "        f\"https://www.gutenberg.org/files/{book_id}/{book_id}.txt\",\n",
    "        f\"https://www.gutenberg.org/cache/epub/{book_id}/pg{book_id}.txt\"\n",
    "    ]\n",
    "    \n",
    "    for url in text_urls:\n",
    "        try:\n",
    "            response = requests.get(url, headers=HEADERS, timeout=TIMEOUT)\n",
    "            if response.status_code == 200:\n",
    "                try:\n",
    "                    return response.content.decode('utf-8')\n",
    "                except UnicodeDecodeError:\n",
    "                    try:\n",
    "                        return response.content.decode('latin-1')\n",
    "                    except UnicodeDecodeError:\n",
    "                        return None\n",
    "        except:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def extract_main_text(raw_text):\n",
    "    if not raw_text:\n",
    "        return None\n",
    "    start_marker = '*** START OF THIS PROJECT GUTENBERG EBOOK'\n",
    "    end_marker = '*** END OF THIS PROJECT GUTENBERG EBOOK'\n",
    "    start = raw_text.upper().find(start_marker)\n",
    "    end = raw_text.upper().find(end_marker)\n",
    "    if start != -1:\n",
    "        raw_text = raw_text[start + len(start_marker):]\n",
    "    if end != -1:\n",
    "        raw_text = raw_text[:end]\n",
    "    return raw_text.strip()\n",
    "\n",
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "    return text.strip()[:500000]\n",
    "\n",
    "def save_to_csv(books, filename=\"books_data.csv\"):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['ID', 'Title', 'Author', 'Cover', 'Text'])\n",
    "        for book in books:\n",
    "            writer.writerow(book)\n",
    "    print(f\"\\nðŸ“š ØªÙ… Ø­ÙØ¸ {len(books)} ÙƒØªØ§Ø¨ ÙÙŠ Ø§Ù„Ù…Ù„Ù: {filename}\")\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ 10 ÙƒØªØ¨\n",
    "books = []\n",
    "\n",
    "for book_id in tqdm(range(1, 11), desc=\"ðŸ“– ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒØªØ¨\"):\n",
    "    title, author, cover_url = get_book_metadata(book_id)\n",
    "    if not title:\n",
    "        continue\n",
    "\n",
    "    raw_text = get_book_text(book_id)\n",
    "    if not raw_text:\n",
    "        continue\n",
    "\n",
    "    main_text = extract_main_text(raw_text)\n",
    "    cleaned = clean_text(main_text)\n",
    "\n",
    "    if not cleaned:\n",
    "        continue\n",
    "\n",
    "    books.append([book_id, title, author, cover_url, cleaned])\n",
    "    time.sleep(2)\n",
    "\n",
    "# Ø­ÙØ¸Ù‡Ù… ÙÙŠ CSV\n",
    "save_to_csv(books)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ“– ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒØªØ¨:  10%|â–ˆ         | 1/10 [00:30<04:32, 30.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error fetching metadata for 1: HTTPSConnectionPool(host='www.gutenberg.org', port=443): Max retries exceeded with url: /ebooks/1 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001A529EE6990>, 'Connection to www.gutenberg.org timed out. (connect timeout=30)'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ“– ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒØªØ¨:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4/10 [01:48<02:43, 27.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error fetching metadata for 4: HTTPSConnectionPool(host='www.gutenberg.org', port=443): Max retries exceeded with url: /ebooks/4 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001A529EE6850>, 'Connection to www.gutenberg.org timed out. (connect timeout=30)'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ“– ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒØªØ¨:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 9/10 [03:28<00:19, 19.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error fetching metadata for 9: HTTPSConnectionPool(host='www.gutenberg.org', port=443): Max retries exceeded with url: /ebooks/9 (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001A529EE7890>, 'Connection to www.gutenberg.org timed out. (connect timeout=30)'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ“– ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒØªØ¨: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [03:34<00:00, 21.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“š ØªÙ… Ø­ÙØ¸ 6 ÙƒØªØ§Ø¨ ÙÙŠ Ø§Ù„Ù…Ù„Ù: books_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'\n",
    "}\n",
    "TIMEOUT = 30\n",
    "\n",
    "def get_book_metadata(book_id):\n",
    "    try:\n",
    "        base_url = f\"https://www.gutenberg.org/ebooks/{book_id}\"\n",
    "        response = requests.get(base_url, headers=HEADERS, timeout=TIMEOUT)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        title = soup.find('h1', {'itemprop': 'name'}).get_text(strip=True) if soup.find('h1', {'itemprop': 'name'}) else f\"Untitled {book_id}\"\n",
    "        author = soup.find('a', {'itemprop': 'creator'}).get_text(strip=True) if soup.find('a', {'itemprop': 'creator'}) else \"Unknown\"\n",
    "        \n",
    "        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù„Ø®Øµ\n",
    "        summary = \"\"\n",
    "        for heading in soup.find_all(['h2', 'h3']):\n",
    "            if 'summary' in heading.get_text().lower() or 'about' in heading.get_text().lower():\n",
    "                next_node = heading.find_next_sibling()\n",
    "                while next_node and next_node.name not in ['h2', 'h3']:\n",
    "                    if next_node.name == 'p':\n",
    "                        summary += next_node.get_text() + \" \"\n",
    "                    next_node = next_node.find_next_sibling()\n",
    "                break\n",
    "        summary = summary.strip() or \"No summary available\"\n",
    "        \n",
    "        cover_img = soup.find('img', {'itemprop': 'image'})\n",
    "        if cover_img:\n",
    "            cover_url = cover_img['src']\n",
    "            if not cover_url.startswith('http'):\n",
    "                cover_url = f\"https://www.gutenberg.org{cover_url}\"\n",
    "        else:\n",
    "            cover_url = f\"https://www.gutenberg.org/cache/epub/{book_id}/pg{book_id}.cover.medium.jpg\"\n",
    "\n",
    "        return title, author, summary, cover_url\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error fetching metadata for {book_id}: {str(e)}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "def get_book_text(book_id):\n",
    "    text_urls = [\n",
    "        f\"https://www.gutenberg.org/files/{book_id}/{book_id}-0.txt\",\n",
    "        f\"https://www.gutenberg.org/files/{book_id}/{book_id}.txt\",\n",
    "        f\"https://www.gutenberg.org/cache/epub/{book_id}/pg{book_id}.txt\"\n",
    "    ]\n",
    "    \n",
    "    for url in text_urls:\n",
    "        try:\n",
    "            response = requests.get(url, headers=HEADERS, timeout=TIMEOUT)\n",
    "            if response.status_code == 200:\n",
    "                try:\n",
    "                    return response.content.decode('utf-8')\n",
    "                except UnicodeDecodeError:\n",
    "                    try:\n",
    "                        return response.content.decode('latin-1')\n",
    "                    except UnicodeDecodeError:\n",
    "                        return None\n",
    "        except:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def extract_main_text(raw_text):\n",
    "    if not raw_text:\n",
    "        return None\n",
    "    start_marker = '*** START OF THIS PROJECT GUTENBERG EBOOK'\n",
    "    end_marker = '*** END OF THIS PROJECT GUTENBERG EBOOK'\n",
    "    start = raw_text.upper().find(start_marker)\n",
    "    end = raw_text.upper().find(end_marker)\n",
    "    if start != -1:\n",
    "        raw_text = raw_text[start + len(start_marker):]\n",
    "    if end != -1:\n",
    "        raw_text = raw_text[:end]\n",
    "    return raw_text.strip()\n",
    "\n",
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "    return text.strip()[:500000]\n",
    "\n",
    "def save_to_csv(books, filename=\"books_data.csv\"):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['ID', 'Title', 'Author', 'Summary', 'Text', 'Cover'])  # Cover Ø£ØµØ¨Ø­ Ø¢Ø®Ø± Ø¹Ù…ÙˆØ¯\n",
    "        for book in books:\n",
    "            writer.writerow(book)\n",
    "    print(f\"\\nðŸ“š ØªÙ… Ø­ÙØ¸ {len(books)} ÙƒØªØ§Ø¨ ÙÙŠ Ø§Ù„Ù…Ù„Ù: {filename}\")\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ 10 ÙƒØªØ¨\n",
    "books = []\n",
    "\n",
    "for book_id in tqdm(range(1, 11), desc=\"ðŸ“– ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒØªØ¨\"):\n",
    "    title, author, summary, cover_url = get_book_metadata(book_id)\n",
    "    if not title:\n",
    "        continue\n",
    "\n",
    "    raw_text = get_book_text(book_id)\n",
    "    if not raw_text:\n",
    "        continue\n",
    "\n",
    "    main_text = extract_main_text(raw_text)\n",
    "    cleaned = clean_text(main_text)\n",
    "\n",
    "    if not cleaned:\n",
    "        continue\n",
    "\n",
    "    books.append([book_id, title, author, summary, cleaned, cover_url])  # Cover Ø£ØµØ¨Ø­ Ø¢Ø®Ø± Ø¹Ù†ØµØ±\n",
    "    time.sleep(2)\n",
    "\n",
    "# Ø­ÙØ¸Ù‡Ù… ÙÙŠ CSV\n",
    "save_to_csv(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ØªÙ… Ø¥Ø¶Ø§ÙØ© Ø¹Ù…ÙˆØ¯ Rating ÙÙ‚Ø· Ø¨Ù†Ø¬Ø§Ø­.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„ÙÙŠÙ†\n",
    "df_main = pd.read_csv(r'C:/Users/BADAWY/Desktop/GP- Project/Data Set/Main_Dataset.csv')\n",
    "df_with_rating = pd.read_csv(r'C:/Users/BADAWY/Desktop/GP- Project/Data Set/Main_Final_with_user_data.csv')\n",
    "\n",
    "# Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙØ±Ø§ØºØ§Øª Ù…Ù† Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©\n",
    "df_main.columns = df_main.columns.str.strip()\n",
    "df_with_rating.columns = df_with_rating.columns.str.strip()\n",
    "\n",
    "# Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙÙ‚Ø· Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©: id Ùˆ Rating\n",
    "rating_data = df_with_rating[['ID', 'Rating']].copy()\n",
    "rating_data.rename(columns={'ID': 'id'}, inplace=True)\n",
    "\n",
    "# Ø¯Ù…Ø¬ Ø¹Ù…ÙˆØ¯ Rating ÙÙ‚Ø· Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ id\n",
    "df_main_with_rating = df_main.merge(rating_data, on='id', how='left')\n",
    "\n",
    "# Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø¬Ø¯ÙŠØ¯\n",
    "df_main_with_rating.to_csv(r'C:/Users/BADAWY/Desktop/GP- Project/Data Set/Main_Dataset_with_rating.csv', index=False)\n",
    "\n",
    "print(\"âœ… ØªÙ… Ø¥Ø¶Ø§ÙØ© Ø¹Ù…ÙˆØ¯ Rating ÙÙ‚Ø· Ø¨Ù†Ø¬Ø§Ø­.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_excel('C:/Users/BADAWY/Desktop/GP- Project/Data Set/Main_Final.xlsx')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
